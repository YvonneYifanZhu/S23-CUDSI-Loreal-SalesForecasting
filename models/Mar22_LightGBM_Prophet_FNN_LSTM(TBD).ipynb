{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9416725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML model\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdda5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data import need to change path if necessary\n",
    "\n",
    "data = pd.read_csv(\"/Users/yvonne_zhu/Desktop/MSDS Capstone/Capstone_EDAV/RData/traffic_cleaned_forPython_k.csv\", parse_dates=['day'])\n",
    "data.set_index('day', inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "### data overview \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f45543",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One hot encoding\n",
    "\n",
    "data = pd.get_dummies(data, dtype=\"bool\")\n",
    "data = data.astype('int')\n",
    "\n",
    "### To aviod \"Do not support special JSON characters in feature name\" error in LightGBM\n",
    "import re\n",
    "data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf16dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for splitting train test\n",
    "\n",
    "def fun_split_train(train_ratio, \n",
    "                           val_ratio, \n",
    "                           test_ratio,\n",
    "                           data,\n",
    "                           target_col = 'CORE_VL_NbEntry'):\n",
    "    \n",
    "    train_size = int(train_ratio * len(data))\n",
    "    val_size = int(val_ratio * len(data))\n",
    "    test_size = len(data) - train_size - val_size\n",
    "    \n",
    "    train_data = data.iloc[:train_size]\n",
    "    val_data = data.iloc[train_size:train_size + val_size]\n",
    "    test_data = data.iloc[-test_size:]\n",
    "    \n",
    "    feature_cols = [col for col in data.columns if col != target_col]\n",
    "    \n",
    "    X_train, y_train = train_data[feature_cols], train_data[target_col]\n",
    "    X_val, y_val = val_data[feature_cols], val_data[target_col]\n",
    "    X_test, y_test = test_data[feature_cols], test_data[target_col]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34c2a31",
   "metadata": {},
   "source": [
    "### Cyclical Features Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6e358",
   "metadata": {},
   "source": [
    "### Sin and Cos on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc6bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin and cos transformations on both test and train set\n",
    "\n",
    "# define function for sin and cos\n",
    "def encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "\n",
    "\n",
    "data_trans = data.reset_index()\n",
    "\n",
    "data_trans['y'] = data_trans.day.dt.year\n",
    "data_trans['m'] = data_trans.day.dt.month\n",
    "data_trans['d'] = data_trans.day.dt.day\n",
    "\n",
    "data_sincos = encode(data_trans, 'm', 12)\n",
    "data_sincos = encode(data_trans, 'd', 31)\n",
    "data_sincos.set_index('day', inplace=True)\n",
    "\n",
    "# not necessary to transform year into sin cos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba3ff80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORE_VL_NbEntry</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>Population</th>\n",
       "      <th>MHI</th>\n",
       "      <th>CORE_LB_State_AZ</th>\n",
       "      <th>CORE_LB_State_CA</th>\n",
       "      <th>CORE_LB_State_CO</th>\n",
       "      <th>CORE_LB_State_CT</th>\n",
       "      <th>CORE_LB_State_DC</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_Thursday</th>\n",
       "      <th>weekday_Tuesday</th>\n",
       "      <th>weekday_Wednesday</th>\n",
       "      <th>y</th>\n",
       "      <th>m</th>\n",
       "      <th>d</th>\n",
       "      <th>m_sin</th>\n",
       "      <th>m_cos</th>\n",
       "      <th>d_sin</th>\n",
       "      <th>d_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>40</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>65913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>71</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>22</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>52</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>25</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CORE_VL_NbEntry  year  month  Population    MHI  CORE_LB_State_AZ  \\\n",
       "day                                                                             \n",
       "2016-01-01               40  2016      1          62  65913                 1   \n",
       "2016-01-01               71  2016      1         253  84097                 0   \n",
       "2016-01-01               22  2016      1         253  84097                 0   \n",
       "2016-01-01               52  2016      1         253  84097                 0   \n",
       "2016-01-01               25  2016      1         253  84097                 0   \n",
       "\n",
       "            CORE_LB_State_CA  CORE_LB_State_CO  CORE_LB_State_CT  \\\n",
       "day                                                                \n",
       "2016-01-01                 0                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "\n",
       "            CORE_LB_State_DC  ...  weekday_Thursday  weekday_Tuesday  \\\n",
       "day                           ...                                      \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "\n",
       "            weekday_Wednesday     y  m  d  m_sin     m_cos     d_sin    d_cos  \n",
       "day                                                                            \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sincos.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caec497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b816e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75f83250",
   "metadata": {},
   "source": [
    "### light GBM model \n",
    "- without transformation\n",
    "- include state and city\n",
    "\n",
    "currently, just excluding state and city has higher RMSE; thus, the below model should replace state variable once we find enough census feature to encode this feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8859892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvonne_zhu/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yvonne_zhu/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 313\n",
      "[LightGBM] [Info] Number of data points in the train set: 62521, number of used features: 125\n",
      "[LightGBM] [Info] Start training from score 37.748117\n",
      "[1]\tvalid_0's rmse: 30.7689\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's rmse: 29.7121\n",
      "[3]\tvalid_0's rmse: 28.7468\n",
      "[4]\tvalid_0's rmse: 27.8386\n",
      "[5]\tvalid_0's rmse: 27.0283\n",
      "[6]\tvalid_0's rmse: 26.2874\n",
      "[7]\tvalid_0's rmse: 25.5898\n",
      "[8]\tvalid_0's rmse: 24.9656\n",
      "[9]\tvalid_0's rmse: 24.3799\n",
      "[10]\tvalid_0's rmse: 23.8456\n",
      "[11]\tvalid_0's rmse: 23.3424\n",
      "[12]\tvalid_0's rmse: 22.8801\n",
      "[13]\tvalid_0's rmse: 22.4377\n",
      "[14]\tvalid_0's rmse: 22.0233\n",
      "[15]\tvalid_0's rmse: 21.6392\n",
      "[16]\tvalid_0's rmse: 21.2895\n",
      "[17]\tvalid_0's rmse: 20.9865\n",
      "[18]\tvalid_0's rmse: 20.7047\n",
      "[19]\tvalid_0's rmse: 20.4286\n",
      "[20]\tvalid_0's rmse: 20.1399\n",
      "[21]\tvalid_0's rmse: 19.9151\n",
      "[22]\tvalid_0's rmse: 19.6792\n",
      "[23]\tvalid_0's rmse: 19.4711\n",
      "[24]\tvalid_0's rmse: 19.2704\n",
      "[25]\tvalid_0's rmse: 19.0878\n",
      "[26]\tvalid_0's rmse: 18.905\n",
      "[27]\tvalid_0's rmse: 18.7488\n",
      "[28]\tvalid_0's rmse: 18.5861\n",
      "[29]\tvalid_0's rmse: 18.4415\n",
      "[30]\tvalid_0's rmse: 18.3019\n",
      "[31]\tvalid_0's rmse: 18.1717\n",
      "[32]\tvalid_0's rmse: 18.0511\n",
      "[33]\tvalid_0's rmse: 17.9152\n",
      "[34]\tvalid_0's rmse: 17.7922\n",
      "[35]\tvalid_0's rmse: 17.7\n",
      "[36]\tvalid_0's rmse: 17.5676\n",
      "[37]\tvalid_0's rmse: 17.4815\n",
      "[38]\tvalid_0's rmse: 17.3983\n",
      "[39]\tvalid_0's rmse: 17.2875\n",
      "[40]\tvalid_0's rmse: 17.1941\n",
      "[41]\tvalid_0's rmse: 17.1236\n",
      "[42]\tvalid_0's rmse: 17.0467\n",
      "[43]\tvalid_0's rmse: 16.9955\n",
      "[44]\tvalid_0's rmse: 16.9297\n",
      "[45]\tvalid_0's rmse: 16.849\n",
      "[46]\tvalid_0's rmse: 16.7804\n",
      "[47]\tvalid_0's rmse: 16.7241\n",
      "[48]\tvalid_0's rmse: 16.664\n",
      "[49]\tvalid_0's rmse: 16.6167\n",
      "[50]\tvalid_0's rmse: 16.5567\n",
      "[51]\tvalid_0's rmse: 16.5086\n",
      "[52]\tvalid_0's rmse: 16.4447\n",
      "[53]\tvalid_0's rmse: 16.4077\n",
      "[54]\tvalid_0's rmse: 16.3535\n",
      "[55]\tvalid_0's rmse: 16.3033\n",
      "[56]\tvalid_0's rmse: 16.2545\n",
      "[57]\tvalid_0's rmse: 16.2044\n",
      "[58]\tvalid_0's rmse: 16.148\n",
      "[59]\tvalid_0's rmse: 16.1046\n",
      "[60]\tvalid_0's rmse: 16.0532\n",
      "[61]\tvalid_0's rmse: 16.0063\n",
      "[62]\tvalid_0's rmse: 15.9716\n",
      "[63]\tvalid_0's rmse: 15.939\n",
      "[64]\tvalid_0's rmse: 15.8915\n",
      "[65]\tvalid_0's rmse: 15.8501\n",
      "[66]\tvalid_0's rmse: 15.83\n",
      "[67]\tvalid_0's rmse: 15.8066\n",
      "[68]\tvalid_0's rmse: 15.7591\n",
      "[69]\tvalid_0's rmse: 15.7122\n",
      "[70]\tvalid_0's rmse: 15.6779\n",
      "[71]\tvalid_0's rmse: 15.6663\n",
      "[72]\tvalid_0's rmse: 15.6377\n",
      "[73]\tvalid_0's rmse: 15.6069\n",
      "[74]\tvalid_0's rmse: 15.5796\n",
      "[75]\tvalid_0's rmse: 15.5688\n",
      "[76]\tvalid_0's rmse: 15.5684\n",
      "[77]\tvalid_0's rmse: 15.5443\n",
      "[78]\tvalid_0's rmse: 15.5134\n",
      "[79]\tvalid_0's rmse: 15.4864\n",
      "[80]\tvalid_0's rmse: 15.4543\n",
      "[81]\tvalid_0's rmse: 15.4352\n",
      "[82]\tvalid_0's rmse: 15.4058\n",
      "[83]\tvalid_0's rmse: 15.3755\n",
      "[84]\tvalid_0's rmse: 15.3447\n",
      "[85]\tvalid_0's rmse: 15.3206\n",
      "[86]\tvalid_0's rmse: 15.3012\n",
      "[87]\tvalid_0's rmse: 15.2906\n",
      "[88]\tvalid_0's rmse: 15.2693\n",
      "[89]\tvalid_0's rmse: 15.2394\n",
      "[90]\tvalid_0's rmse: 15.215\n",
      "[91]\tvalid_0's rmse: 15.1925\n",
      "[92]\tvalid_0's rmse: 15.1883\n",
      "[93]\tvalid_0's rmse: 15.166\n",
      "[94]\tvalid_0's rmse: 15.1451\n",
      "[95]\tvalid_0's rmse: 15.123\n",
      "[96]\tvalid_0's rmse: 15.1047\n",
      "[97]\tvalid_0's rmse: 15.0887\n",
      "[98]\tvalid_0's rmse: 15.0769\n",
      "[99]\tvalid_0's rmse: 15.0569\n",
      "[100]\tvalid_0's rmse: 15.0411\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 15.0411\n"
     ]
    }
   ],
   "source": [
    "### try on light gbm model\n",
    "\n",
    "### split using data without transformation\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = fun_split_train(0.7, 0.2, 0.1, data)\n",
    "                                                                 \n",
    "\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb, free_raw_data=False)\n",
    "\n",
    "\n",
    "### hyperparameter  - tunning\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 100}\n",
    "\n",
    "\n",
    "### model fitting\n",
    "model = lgb.train(params, train_lgb, valid_sets=[val_lgb], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007d9c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 13.96\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bae1f8",
   "metadata": {},
   "source": [
    "### light GBM model \n",
    "- with sin and cos transformation, RMSE is slightly higher than without transformation\n",
    "- include state and city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea6f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvonne_zhu/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/yvonne_zhu/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 440\n",
      "[LightGBM] [Info] Number of data points in the train set: 62521, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 37.748117\n",
      "[1]\tvalid_0's rmse: 30.7667\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's rmse: 29.7167\n",
      "[3]\tvalid_0's rmse: 28.7322\n",
      "[4]\tvalid_0's rmse: 27.844\n",
      "[5]\tvalid_0's rmse: 27.0294\n",
      "[6]\tvalid_0's rmse: 26.2411\n",
      "[7]\tvalid_0's rmse: 25.5253\n",
      "[8]\tvalid_0's rmse: 24.8471\n",
      "[9]\tvalid_0's rmse: 24.2279\n",
      "[10]\tvalid_0's rmse: 23.6515\n",
      "[11]\tvalid_0's rmse: 23.0983\n",
      "[12]\tvalid_0's rmse: 22.6358\n",
      "[13]\tvalid_0's rmse: 22.1524\n",
      "[14]\tvalid_0's rmse: 21.7486\n",
      "[15]\tvalid_0's rmse: 21.3617\n",
      "[16]\tvalid_0's rmse: 20.9468\n",
      "[17]\tvalid_0's rmse: 20.5462\n",
      "[18]\tvalid_0's rmse: 20.2264\n",
      "[19]\tvalid_0's rmse: 19.8715\n",
      "[20]\tvalid_0's rmse: 19.5537\n",
      "[21]\tvalid_0's rmse: 19.2498\n",
      "[22]\tvalid_0's rmse: 19.0055\n",
      "[23]\tvalid_0's rmse: 18.7614\n",
      "[24]\tvalid_0's rmse: 18.497\n",
      "[25]\tvalid_0's rmse: 18.2761\n",
      "[26]\tvalid_0's rmse: 18.0906\n",
      "[27]\tvalid_0's rmse: 17.8972\n",
      "[28]\tvalid_0's rmse: 17.6993\n",
      "[29]\tvalid_0's rmse: 17.5292\n",
      "[30]\tvalid_0's rmse: 17.3684\n",
      "[31]\tvalid_0's rmse: 17.2142\n",
      "[32]\tvalid_0's rmse: 17.057\n",
      "[33]\tvalid_0's rmse: 16.9316\n",
      "[34]\tvalid_0's rmse: 16.8003\n",
      "[35]\tvalid_0's rmse: 16.653\n",
      "[36]\tvalid_0's rmse: 16.502\n",
      "[37]\tvalid_0's rmse: 16.3918\n",
      "[38]\tvalid_0's rmse: 16.2938\n",
      "[39]\tvalid_0's rmse: 16.1949\n",
      "[40]\tvalid_0's rmse: 16.0779\n",
      "[41]\tvalid_0's rmse: 15.9809\n",
      "[42]\tvalid_0's rmse: 15.8886\n",
      "[43]\tvalid_0's rmse: 15.7938\n",
      "[44]\tvalid_0's rmse: 15.7041\n",
      "[45]\tvalid_0's rmse: 15.6209\n",
      "[46]\tvalid_0's rmse: 15.5419\n",
      "[47]\tvalid_0's rmse: 15.4939\n",
      "[48]\tvalid_0's rmse: 15.4275\n",
      "[49]\tvalid_0's rmse: 15.3593\n",
      "[50]\tvalid_0's rmse: 15.2784\n",
      "[51]\tvalid_0's rmse: 15.2111\n",
      "[52]\tvalid_0's rmse: 15.1451\n",
      "[53]\tvalid_0's rmse: 15.0897\n",
      "[54]\tvalid_0's rmse: 15.0222\n",
      "[55]\tvalid_0's rmse: 14.9623\n",
      "[56]\tvalid_0's rmse: 14.9002\n",
      "[57]\tvalid_0's rmse: 14.8474\n",
      "[58]\tvalid_0's rmse: 14.8036\n",
      "[59]\tvalid_0's rmse: 14.762\n",
      "[60]\tvalid_0's rmse: 14.7152\n",
      "[61]\tvalid_0's rmse: 14.6688\n",
      "[62]\tvalid_0's rmse: 14.6072\n",
      "[63]\tvalid_0's rmse: 14.5759\n",
      "[64]\tvalid_0's rmse: 14.5277\n",
      "[65]\tvalid_0's rmse: 14.4978\n",
      "[66]\tvalid_0's rmse: 14.4502\n",
      "[67]\tvalid_0's rmse: 14.3975\n",
      "[68]\tvalid_0's rmse: 14.3747\n",
      "[69]\tvalid_0's rmse: 14.3351\n",
      "[70]\tvalid_0's rmse: 14.303\n",
      "[71]\tvalid_0's rmse: 14.2919\n",
      "[72]\tvalid_0's rmse: 14.2852\n",
      "[73]\tvalid_0's rmse: 14.2538\n",
      "[74]\tvalid_0's rmse: 14.2207\n",
      "[75]\tvalid_0's rmse: 14.1825\n",
      "[76]\tvalid_0's rmse: 14.1668\n",
      "[77]\tvalid_0's rmse: 14.1359\n",
      "[78]\tvalid_0's rmse: 14.0983\n",
      "[79]\tvalid_0's rmse: 14.0749\n",
      "[80]\tvalid_0's rmse: 14.0573\n",
      "[81]\tvalid_0's rmse: 14.0382\n",
      "[82]\tvalid_0's rmse: 14.0147\n",
      "[83]\tvalid_0's rmse: 14\n",
      "[84]\tvalid_0's rmse: 13.971\n",
      "[85]\tvalid_0's rmse: 13.9445\n",
      "[86]\tvalid_0's rmse: 13.9298\n",
      "[87]\tvalid_0's rmse: 13.9003\n",
      "[88]\tvalid_0's rmse: 13.8637\n",
      "[89]\tvalid_0's rmse: 13.8325\n",
      "[90]\tvalid_0's rmse: 13.8244\n",
      "[91]\tvalid_0's rmse: 13.7917\n",
      "[92]\tvalid_0's rmse: 13.7656\n",
      "[93]\tvalid_0's rmse: 13.7599\n",
      "[94]\tvalid_0's rmse: 13.735\n",
      "[95]\tvalid_0's rmse: 13.7235\n",
      "[96]\tvalid_0's rmse: 13.6836\n",
      "[97]\tvalid_0's rmse: 13.664\n",
      "[98]\tvalid_0's rmse: 13.6369\n",
      "[99]\tvalid_0's rmse: 13.6111\n",
      "[100]\tvalid_0's rmse: 13.6054\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 13.6054\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = fun_split_train(0.7, 0.2, 0.1, data_sincos)\n",
    "                                                                 \n",
    "\n",
    "train_lgb = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "val_lgb = lgb.Dataset(X_val, label=y_val, reference=train_lgb, free_raw_data=False)\n",
    "\n",
    "\n",
    "### hyperparameter  - tunning\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 100}\n",
    "\n",
    "\n",
    "### model fitting\n",
    "model = lgb.train(params, train_lgb, valid_sets=[val_lgb], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd8b341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 14.17\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c946e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras\n",
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd6f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc6d46af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1954/1954 [==============================] - 3s 1ms/step - loss: 233705.9062 - val_loss: 1060.7114\n",
      "Epoch 2/5\n",
      "1954/1954 [==============================] - 3s 1ms/step - loss: 1037.7701 - val_loss: 1027.2321\n",
      "Epoch 3/5\n",
      "1954/1954 [==============================] - 3s 1ms/step - loss: 1036.2791 - val_loss: 1019.8461\n",
      "Epoch 4/5\n",
      "1954/1954 [==============================] - 3s 1ms/step - loss: 1053.4358 - val_loss: 1018.9901\n",
      "Epoch 5/5\n",
      "1954/1954 [==============================] - 2s 1ms/step - loss: 1063.2135 - val_loss: 1055.6604\n",
      "280/280 [==============================] - 0s 991us/step\n",
      "Epoch 1/5\n",
      "1954/1954 [==============================] - 3s 1ms/step - loss: 1050.2981 - val_loss: 1060.4423\n",
      "Epoch 2/5\n",
      "1954/1954 [==============================] - 2s 1ms/step - loss: 1029.1605 - val_loss: 1009.4909\n",
      "Epoch 3/5\n",
      "1954/1954 [==============================] - 2s 1ms/step - loss: 1023.2253 - val_loss: 1004.4053\n",
      "Epoch 4/5\n",
      "1954/1954 [==============================] - 2s 1ms/step - loss: 1015.6573 - val_loss: 1007.1956\n",
      "Epoch 5/5\n",
      "1954/1954 [==============================] - 2s 1ms/step - loss: 1010.0486 - val_loss: 990.6738\n",
      "280/280 [==============================] - 0s 756us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def train_model_dense(X_train, y_train, X_val, y_val, epochs):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(10, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "            Dense(10, activation=\"relu\"),\n",
    "            Dense(10, activation=\"relu\"),\n",
    "            Dense(1, activation=\"linear\")\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=Adam(), loss=\"mean_squared_error\")\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# build model on unencoded data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = fun_split_train(0.7, 0.2, 0.1, data)\n",
    "unencode_model, unencode_history = train_model_dense(X_train, y_train, X_val, y_val, epochs=5)\n",
    "\n",
    "mse_unencoded = mean_squared_error(y_test, unencode_model.predict(X_test))\n",
    "\n",
    "\n",
    "# build model on encoded data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = fun_split_train(0.7, 0.2, 0.1, data_sincos)\n",
    "encode_model, encode_history = train_model_dense(X_train, y_train, X_val, y_val, epochs=5)\n",
    "\n",
    "mse_encoded = mean_squared_error(y_test, encode_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e50ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAAsTAAALEwEAmpwYAAAhl0lEQVR4nO3deZRU1bn38e9DM8igAnYLCmg3ikT0KmqLxCFRgoLGK8nNcFWi4BCigm3yZr2J5mbFTO+7ssxd8Y2CAw5RIwLGKWg0BhVD4pWhUUTAqQMqIKMQREYbnvePfTpd3XTT1U1Vnao6v89aZ3X1Prurnj7Qzz6193NOmbsjIiLJ0C7uAEREJHeU9EVEEkRJX0QkQZT0RUQSRElfRCRB2scdwL6UlpZ6eXl53GGIiBSUBQsWbHD3sqb25XXSLy8vp7q6Ou4wREQKipl90Nw+Te+IiCSIkr6ISIIo6YuIJIiSvohIgijpi4gkSItJ38z6mdksM1tqZkvM7Iao/ddm9raZLTKzJ82se8rP3GRmNWb2jpmNSGkfGbXVmNmNWfmNRESkWemc6dcC33f3QcBQYLyZDQJmAse7+wnAu8BNANG+i4HjgJHAHWZWYmYlwCTgfGAQcEnUV0REcqTFpO/uq939tejxFuAtoI+7/8Xda6Nuc4C+0eNRwDR33+nuy4EaYEi01bj7MnffBUyL+mbcpk1w882wdGk2nl1EpHC1ak7fzMqBk4C5jXZdCTwXPe4DrEjZtzJqa6698WuMM7NqM6tev359a8L7l9274ZZb4Lbb2vTjIiJFK+2kb2bdgMeB77r7Jynt/0WYApqSiYDcfbK7V7p7ZVlZk1cRt6i0FEaPhoceCmf9IiISpJX0zawDIeFPcfcnUtrHAhcCo73+I7hWAf1Sfrxv1NZce1Zcfz1s3w733ZetVxARKTzpVO8YcB/wlrv/JqV9JPAD4CJ335byIzOAi82sk5lVAAOAecB8YICZVZhZR8Ji74zM/SoNnXgifPGLMHEi1Na23F9EJAnSOdM/A7gMGGZmC6PtAmAicCAwM2q7C8DdlwCPAkuBPwPj3X13tOg7AXiesBj8aNQ3a6qq4IMP4Omns/kqIiKFw/L5g9ErKyt9f+6yWVsLRx0F/fvDrFkZDExEJI+Z2QJ3r2xqX1Ffkdu+PYwfDy+/DIsWxR2NiEj8ijrpA1x9NXTuDLffHnckIiLxK/qk37MnXHYZPPwwfPxx3NGIiMSr6JM+hPLNHTvg3nvjjkREJF6JSPrHHw/DhsGkSSrfFJFkS0TSh1C+uWIF/PGPcUciIhKfxCT9Cy+E8nLdj0dEki0xSb+kBCZMgNmzYeHCuKMREYlHYpI+wJVXQpcuKt8UkeRKVNLv0QMuvxymTIE23rVZRKSgJSrpQyjf3LkT7rkn7khERHIvcUl/0CAYPhzuuAM++yzuaEREcitxSR9C+eaqVfDkk3FHIiKSW4lM+hdcEO68qfJNEUmaRCb9kpIwt//KK7BgQdzRiIjkTiKTPsAVV0DXrirfFJFkSWzSP/hgGDsWpk6FdevijkZEJDcSm/QhXKG7axdMnhx3JCIiuZHopP+5z8GIESrfFJHkSHTSh1C+uXo1PP543JGIiGRf4pP+yJFw9NEq3xSRZEh80m/XLpRvvvoqzJ8fdzQiItnVYtI3s35mNsvMlprZEjO7IWrvaWYzzey96GuPqN3M7DYzqzGzRWZ2cspzjYn6v2dmY7L3a7XO2LHQrZvO9kWk+KVzpl8LfN/dBwFDgfFmNgi4EXjR3QcAL0bfA5wPDIi2ccCdEAYJ4GbgNGAIcHPdQBG3gw4KdfvTp8OaNXFHIyKSPS0mfXdf7e6vRY+3AG8BfYBRwINRtweBr0SPRwEPeTAH6G5mhwEjgJnuvtHdNwEzgZGZ/GX2x4QJoYLn7rvjjkREJHtaNadvZuXAScBcoJe7r452rQF6RY/7ACtSfmxl1NZce+PXGGdm1WZWvT6HN70/5hg4/3y4885Quy8iUozSTvpm1g14HPiuu3+Sus/dHfBMBOTuk9290t0ry8rKMvGUabvhBli7Fv7wh5y+rIhIzqSV9M2sAyHhT3H3J6LmtdG0DdHXupsZrAL6pfx436itufa8ce65MHCgFnRFpHilU71jwH3AW+7+m5RdM4C6CpwxwB9T2i+PqniGApujaaDngfPMrEe0gHte1JY36so3582DuXPjjkZEJPPSOdM/A7gMGGZmC6PtAuBXwLlm9h4wPPoe4FlgGVAD3ANcB+DuG4FfAPOj7edRW165/PJQzaOzfREpRham4/NTZWWlV1dX5/x1v/c9mDgRPvgADj885y8vIrJfzGyBu1c2tS/xV+Q2Zfx42L1b5ZsiUnyU9Jtw9NHw5S/DXXfBzp1xRyMikjlK+s2oqgofrjJ9etyRiIhkjpJ+M4YPh2OPDQu6ebzsISLSKkr6zTAL5ZsLFoQ7cIqIFAMl/X247LLwWboq3xSRYqGkvw/dusFVV8Fjj8HKlXFHIyKy/5T0WzBhAuzZEyp5REQKnZJ+Cyoq4KKLQs3+jh1xRyMisn+U9NNQVQUbNsC0aXFHIiKyf5T003DOOXDccSrfFJHCp6SfBrNwtv/66/DKK3FHIyLSdkr6aRo9Gnr0UPmmiBQ2Jf00de0KV18NTzwBK1a03F9EJB8p6bfCddeFOf077og7EhGRtlHSb4Xychg1CiZPhu3b445GRKT1lPRbqaoKNm6ERx6JOxIRkdZT0m+lL34R/u3fVL4pIoVJSb+V6so3Fy2C2bPjjkZEpHWU9Ntg9Gjo2VPlmyJSeJT026BzZxg3Dp56Knx4uohIoVDSb6Nrrw1TPSrfFJFC0mLSN7P7zWydmS1OaRtsZnPMbKGZVZvZkKjdzOw2M6sxs0VmdnLKz4wxs/eibUx2fp3cOeII+OpX4Z57YNu2uKMREUlPOmf6DwAjG7XdAvzM3QcDP4m+BzgfGBBt44A7AcysJ3AzcBowBLjZzHrsZ+yxq6qCTZtgypS4IxERSU+LSd/dZwMbGzcDB0WPDwY+ih6PAh7yYA7Q3cwOA0YAM919o7tvAmay90BScM48EwYPVvmmiBSOts7pfxf4tZmtAP4buClq7wOk3plmZdTWXPtezGxcNGVUvX79+jaGlxt15ZuLF8OsWXFHIyLSsrYm/WuB77l7P+B7wH2ZCsjdJ7t7pbtXlpWVZepps+aSS6C0VOWbIlIY2pr0xwBPRI//QJinB1gF9Evp1zdqa6694B1wQCjfnDEDli+POxoRkX1ra9L/CPhi9HgY8F70eAZweVTFMxTY7O6rgeeB88ysR7SAe17UVhSuvRbatYNJk+KORERk39q31MHMpgJnA6VmtpJQhfNt4Ldm1h7YQajUAXgWuACoAbYBVwC4+0Yz+wUwP+r3c3dvvDhcsPr2ha99De69F376U+jWLe6IRESaZp7HZSeVlZVeXV0ddxhpeeWVUM1z551wzTVxRyMiSWZmC9y9sql9uiI3Q04/HU45ReWbIpLflPQzpK5886234MUX445GRKRpSvoZ9J//CYceqvJNEclfSvoZ1KkTfOc78Mwz8I9/xB2NiMjelPQz7JproKRE5Zsikp+U9DPs8MPhG9+A++6DTz+NOxoRkYaU9LOgqgo++QQefDDuSEREGlLSz4LTToNTT4Xbb4c9e+KORkSknpJ+FtSVb77zDsycGXc0IiL1lPSz5BvfgF69VL4pIvlFST9LOnUKlTzPPgvvvddyfxGRXFDSz6JrroEOHWDixLgjEREJlPSzqHfvcJXu734XqnlEROKmpJ9lVVWwZYvKN0UkPyjpZ9mpp8LQoSrfFJH8oKSfA1VVYTH3+aL5rDARKVRK+jnwta/BYYepfFNE4qeknwMdO4bP0f3zn8MFWyIicVHSz5Fx40Lyv/32uCMRkSRT0s+RXr3g4ovhgQdg8+a4oxGRpFLSz6GqKti6NdTti4jEQUk/h045JXyA+u23w+7dcUcjIknUYtI3s/vNbJ2ZLW7Ufr2ZvW1mS8zslpT2m8ysxszeMbMRKe0jo7YaM7sxs79G4aiqgmXL4Lnn4o5ERJIonTP9B4CRqQ1mdg4wCjjR3Y8D/jtqHwRcDBwX/cwdZlZiZiXAJOB8YBBwSdQ3cf7jP6BPH5Vvikg8Wkz67j4b2Nio+VrgV+6+M+qzLmofBUxz953uvhyoAYZEW427L3P3XcC0qG/idOgA110X7rO/dGnc0YhI0rR1Tv8Y4Cwzm2tmfzWzU6P2PsCKlH4ro7bm2vdiZuPMrNrMqtevX9/G8PLbt78dbr2su2+KSK61Nem3B3oCQ4H/DTxqZpaJgNx9srtXuntlWVlZJp4y75SVwaWXhpuw/fOfcUcjIknS1qS/EnjCg3nAHqAUWAX0S+nXN2prrj2xrr8etm2D+++POxIRSZK2Jv2ngHMAzOwYoCOwAZgBXGxmncysAhgAzAPmAwPMrMLMOhIWe2fsZ+wF7aST4KyzwhSPyjdFJFfSKdmcCrwKDDSzlWZ2FXA/0D8q45wGjInO+pcAjwJLgT8D4919t7vXAhOA54G3gEejvolWVQXLl8Of/hR3JCKSFObuccfQrMrKSq+uro47jKyprYWKCjjmGHjxxbijEZFiYWYL3L2yqX26IjdG7dvD+PHw0kuweHHL/UVE9peSfsyuvhoOOEB33xSR3FDSj1lpKYweDb//PWxsfAmciEiGKenngaoq2L4d7rsv7khEpNgp6eeBE06As88O5Zu1tXFHIyLFTEk/T1RVwYcfwtNPxx2JiBQzJf088e//Dkceqbtvikh2KennibryzZdfhkWL4o5GRIqVkn4eueoq6NxZ5Zsikj1K+nmkZ0+47DJ4+GH4+OO4oxGRYqSkn2euvx527IB77407EhEpRkr6eeb442HYMJg0SeWbIpJ5Svp5qKoKVqyAp56KOxIRKTZK+nnowguhvFzlmyKSeUr6eaikBCZMgL/9DV5/Pe5oRKSYKOnnqSuvhC5dVL4pIpmlpJ+nevSAMWPgkUdg/fq4oxGRYqGkn8cmTICdO+Gee+KORESKhZJ+Hhs0CM49F+64Az77LO5oRKQYKOnnuaoqWLUKnnwy7khEpBgo6ee5Cy6Ao45S+aaIZIaSfp5r1y7M7b/yCixYEHc0IlLoWkz6Zna/ma0zs8VN7Pu+mbmZlUbfm5ndZmY1ZrbIzE5O6TvGzN6LtjGZ/TWK2xVXQNeuKt8Ukf2Xzpn+A8DIxo1m1g84D/gwpfl8YEC0jQPujPr2BG4GTgOGADebWY/9CTxJDj4Yxo6FqVNh7dq4oxGRQtZi0nf32cDGJnbdCvwA8JS2UcBDHswBupvZYcAIYKa7b3T3TcBMmhhIpHkTJsCuXTB5ctyRiEgha9OcvpmNAla5+xuNdvUBVqR8vzJqa669qeceZ2bVZla9Xlcl/cvnPgcjRsCdd4bkLyLSFq1O+mbWBfgR8JPMhwPuPtndK929sqysLBsvUbCqqmD1anj88bgjEZFC1ZYz/aOACuANM3sf6Au8Zma9gVVAv5S+faO25tqlFUaOhAEDVL4pIm3X6qTv7m+6+6HuXu7u5YSpmpPdfQ0wA7g8quIZCmx299XA88B5ZtYjWsA9L2qTVmjXLnyy1pw5MG9e3NGISCFKp2RzKvAqMNDMVprZVfvo/iywDKgB7gGuA3D3jcAvgPnR9vOoTVppzBg48ECVb4pI25i7t9wrJpWVlV5dXR13GHnnhhvCgu6HH0Lv3nFHIyL5xswWuHtlU/t0RW4BmjAh3IDt7rvjjkRECo2SfgEaMCDck0flmyLSWkr6BaqqKlyd+4c/xB2JiBQSJf0Cde65MHCgyjdFpHWU9AtUXfnmvHmhhFNEJB1K+gXs8svhoIN0ti8i6VPSL2AHHghXXhnm9T/6KO5oRKQQKOkXuPHjYfduuOuuuCMRkUKgpF/gjj4avvzlkPR37ow7GhHJd0r6ReCGG2D9epg+Pe5IRCTfKekXgS99CY49Nizo5vFdNUQkDyjpFwGzcLHWggXw6qtxRyMi+UxJv0hcdln4LF2Vb4rIvijpF4muXeHqq+Gxx2DlyrijEZF8paRfRMaPhz17VL4pIs1T0i8iFRVw0UXhlss7dsQdjYjkIyX9IlNVBRs2wNSpcUciIvlISb/InHMOHHecyjdFpGlK+kWmrnxz4UL4+9/jjkZE8o2SfhEaPRp69FD5pojsTUm/CNWVbz75ZPjwdBGROkr6RWr8+DCnf+edcUciIvmkxaRvZveb2TozW5zS9msze9vMFpnZk2bWPWXfTWZWY2bvmNmIlPaRUVuNmd2Y8d9EGjjySPjKV2DyZNi+Pe5oRCRfpHOm/wAwslHbTOB4dz8BeBe4CcDMBgEXA8dFP3OHmZWYWQkwCTgfGARcEvWVLKqqgo0b4ZFH4o5ERPJFi0nf3WcDGxu1/cXda6Nv5wB9o8ejgGnuvtPdlwM1wJBoq3H3Ze6+C5gW9ZUs+sIX4IQTVL4pIvUyMad/JfBc9LgPsCJl38qorbn2vZjZODOrNrPq9evXZyC85Kor31y0CGbPjjsaEckH+5X0zey/gFpgSmbCAXef7O6V7l5ZVlaWqadNrEsvhZ49Vb4pIkGbk76ZjQUuBEa7/2vyYBXQL6Vb36ituXbJss6dYdw4eOop+OCDuKMRkbi1Kemb2UjgB8BF7r4tZdcM4GIz62RmFcAAYB4wHxhgZhVm1pGw2Dtj/0KXdF17bZjqmTQp7khEJG7plGxOBV4FBprZSjO7CpgIHAjMNLOFZnYXgLsvAR4FlgJ/Bsa7++5o0XcC8DzwFvBo1Fdy4Igj4KtfhXvuga1b445GROJknsdlHZWVlV5dXR13GEXhb38L1Tx33x2me0SkeJnZAnevbGqfrshNiDPPhMGDVb4pknRK+glRV765ZAnMmhV3NCISFyX9BLnkEigtVfmmSJIp6SfIAQfAd74DM2bA8uVxRyMicVDST5hrr4V27WDS7XtgxQr461/hgQfgZz8L5T1vvAG1tS0+j4gUpvZxByBZ4h7utrZ8OSxbFr4uX06fZcv4eucq7r31TH5667F0o4kazs6d4ZRTYMiQ+q28PCwMiEhBU9IvZNu2wfvv/yuhpyZ3li2DLVsa9j/kEOjfn6ohc5n+0oU8fOlzXDN2B1RUQL9+4cx/3rz6bdIk+M1vws+WljYcBE49NbSJSEFRnX4+270bVq5sOqEvXw5r1jTs37lzSOAVFdC//96PDzwQCG8CTj01jBlLluzjBH7XLli8uOFAsHRpfc1n//4NB4KTToIuXbJ3PEQkLfuq01fSj5M7bNjQ/Jn6hx82nF9v1y5cXttUYq+ogF690p6CeeghGDMG/vIXOPfcVsS8ZQssWNBwIFgR3UC1pCTcyzl1IDj22NAuIjmjpB+nrVvDFExTZ+rLl8OnnzbsX1bW/Jl6v37QoUNGwtq5M4wfQ4bA00/v55OtXg3z5zccCDZvDvu6doXKyoYDQb9+Wh8QySIl/WyqrQ1nus2dra9b17B/ly5NJ/S6rVu3nIX+k5/AL38J774LRx+dwSfeswdqahoOAq+/HqaLILwjSR0EKivD/Z9FJCOU9PeHO6xf3/yZ+ocfhrn3OiUl9VMwTSX3srK8Ocv96KPwWboTJsCtt2b5xXbuhDffhLlz6weCt9+u3z9gQMOBYPDgcGGBiLSakn5LPv20+TP15cvDimeqXr2aP1Pv1w/aF05R1KWXwp/+FNaLo3Xe3Nm8GaqrG74j+OijsK99ezjxxIYDwcCBWh8QSYOS/mefhTPyphL78uXhTD5Vt27Nn6mXl4d56iIxZw58/vMwcSKMHx93NMCqVQ0Hgfnz60tPDzxw7/WBPn3y5p2TSL5IXtLfsAF++MP65L5iRZhnrtO+fZjXaK688ZBDEpVITjstnHQvXRoKhPLKnj3wzjsNB4I33ggDOcBhh+29PtC9e6whi8QteUl/69awMtncgmmfPgU1BZNtU6bAt74VFnVHjw5vZvLajh0h8acOBO++W79/4MCGA8GJJ0KnTvHFK5JjyUv60iq7dsHpp4fye4CjjoLhw0P9/jnnFEhhzaZNDdcH5s6FtWvDvg4dwsJw6kBwzDF5+LZGJIV7m2cclPSlRe6hmGbmTHjhBXj55TCVbhZuwzN8eNjOOKNAimrcw+p06ruB6ur66yIOOihclpw6EBx+eLwxS/HbsyfcE2vt2nBF/b6+HnssvPRSm15GSV9a7bPPwhrqCy+EgWDOnHBJwgEHwFln1Q8CgwcX0Anz7t1hZEsdCBYtqr/quU+fvdcHDjoo3pgl/7nDP//ZchJfsyZct9PUXWw7dQpVgb171389/vjwyUdtoKQv+23LFpg9OwwCL7wQbskDYc37S1+qHwQqKuKNs9W2b4eFCxsOBDU1YZ9ZONtKfUdwwgnQsWOsIUsOuMMnnzSfwBs/rrvwMFWHDiGBN07mTX09+OCMFo8o6UvGrV4NL75YPwisWhXa+/evHwCGDQuDQsH5+OO91wfqyno7dapfHzjyyFDe261bKOOte5y6de0a3h4lqBosb7mH6b10plbWrAkXFDZWUgKHHtpyEu/dG3r0iO3fXUlfsso9VFXWDQCzZoWTJDM4+eSG6wGdO8cdbRu4h+s8Gq8PNL5orznt2u09EDQ3QKTTVteeofswFbxt29JL4mvXNv1v1q5duFI+nTPyQw4piPnM/Ur6ZnY/cCGwzt2Pj9p6AtOBcuB94JvuvsnMDPgtcAGwDRjr7q9FPzMG+HH0tL909wdbClxJvzDV1tavB7zwArz6algjOOAAOPPMhusBBXuB7Z494axx69bwtfHWVHtLfbdsad2nlnXsmPnBpEuX/EhqO3akf0be+KaFEM44SkvTS+SlpQX8H7Fp+5v0vwB8CjyUkvRvATa6+6/M7Eagh7v/0MwuAK4nJP3TgN+6+2nRIFENVAIOLABOcfdN+3ptJf3i8OmnDdcD3nwztPfsGaaA6spD+/ePN868sGtXZgaTxm2teUffpUvmB5NOncLI33g+vLmvdXdpbaxnz/SmVsrKEn0tzn5P75hZOfBMStJ/Bzjb3Veb2WHAy+4+0Mzujh5PTe1Xt7n7d6L2Bv2ao6RfnNasCZVodZVBK1eG9oqKhusB+mCuDHEPC9ZtfQfSXNv27enHUFLS8MaEqbp3T++M/NBDtYiepn0l/bYOhb3cfXX0eA3QK3rcB1iR0m9l1NZce1PBjgPGARxxxBFtDE/yWe/e4UZvl14a8tG779a/C5g+PXw+u1n4IK66QeDMMwt0PSAfmIWz90x/qtnu3WEQSHfQ6NQp/OOnJvNDDy2QCz+Kx36//3F3N7OMrQa7+2RgMoQz/Uw9r+Qns3DXhIEDww3famvDGmndIHDrrXDLLSFfnHFGmAYaPjwMCEU2DVt4SkrCdQy6lqGgtHXFZm00rUP0te6TQlYB/VL69Y3ammsXaaB9exg6FH7843BV8KZN8NxzYUDYsAFuuimUzZeVwde/DnfdBf/4R+umrEWSrK1n+jOAMcCvoq9/TGmfYGbTCAu5m6N5/+eB/2tmPaJ+5wE3tT1sSYquXWHkyLBBWONLXQ94/PHQXl7ecD2grCy2kEXyWjrVO1MJC7GlwFrgZuAp4FHgCOADQsnmxqhkcyIwklCyeYW7V0fPcyXwo+hp/4+7/66l4LSQK/viDu+9Vz8V9NJL9UUfgwfXTwWdeWbmp7NF8pkuzpJEqK2F116rv2nc//xPqIDs2DGsB9SVhp58stYDpLgp6Usibd0Kf/97/TuBhQtDe/fu9dcHDB8ePnpBd0mQYpKNkk2RvNe1K4wYETYINzhMXQ944onQfsQR9VNBw4aFKkKRYqUzfUkk91D1UzcV9NJL4e64ED5oq24q6KyztB4ghUfTOyIt2L07rAfUvQt45ZX69YDTT6+fCqqs1HqA5D8lfZFW2rat4XrA66+H9oMPrl8POPLI+v51awL59jUfYkj9ur9bu3aZfZ5ipTl9kVbq0gXOOy9sEG6nP2tWeBcwcyY8+WS88Unm5NtAVLcNHgxT93l3srZR0hdJQ1kZfPObYXOH998PVwjXvVFOfcPcuC3ur/kQg3vmtj178ut5shVTtu46q6Qv0kpm4Y6gBffRkCK0/d47IiJSgJT0RUQSRElfRCRBlPRFRBJESV9EJEGU9EVEEkRJX0QkQZT0RUQSJK/vvWNm6wmfzNVWpcCGDIWTSYqrdRRX6yiu1inGuI509yY/NDSvk/7+MrPq5m46FCfF1TqKq3UUV+skLS5N74iIJIiSvohIghR70p8cdwDNUFyto7haR3G1TqLiKuo5fRERaajYz/RFRCSFkr6ISIIUfNI3s5Fm9o6Z1ZjZjU3s72Rm06P9c82sPE/iGmtm681sYbRdnaO47jezdWa2uJn9Zma3RXEvMrOT8ySus81sc8rx+kmO4upnZrPMbKmZLTGzG5rok/NjlmZcOT9mZnaAmc0zszeiuH7WRJ+c/02mGVcsf5PRa5eY2etm9kwT+zJ7vNy9YDegBPgH0B/oCLwBDGrU5zrgrujxxcD0PIlrLDAxhmP2BeBkYHEz+y8AngMMGArMzZO4zgaeieF4HQacHD0+EHi3iX/LnB+zNOPK+TGLjkG36HEHYC4wtFGfOP4m04krlr/J6LX/F/BIU/9emT5ehX6mPwSocfdl7r4LmAaMatRnFPBg9Pgx4EtmZnkQVyzcfTawcR9dRgEPeTAH6G5mh+VBXLFw99Xu/lr0eAvwFtCnUbecH7M048q56Bh8Gn3bIdoaV4vk/G8yzbhiYWZ9gS8D9zbTJaPHq9CTfh9gRcr3K9n7P/6/+rh7LbAZOCQP4gL4WjQd8JiZ9ctyTOlKN/Y4fD56e/6cmR2X6xeP3lafRDhLTBXrMdtHXBDDMYumKhYC64CZ7t7s8crh32Q6cUE8f5P/D/gBsKeZ/Rk9XoWe9AvZ00C5u58AzKR+JJemvUa4n8iJwO3AU7l8cTPrBjwOfNfdP8nla+9LC3HFcszcfbe7Dwb6AkPM7PhcvG5L0ogr53+TZnYhsM7dF2T7teoUetJfBaSOxn2jtib7mFl74GDg47jjcveP3X1n9O29wClZjild6RzTnHP3T+renrv7s0AHMyvNxWubWQdCYp3i7k800SWWY9ZSXHEes+g1/wnMAkY22hXH32SLccX0N3kGcJGZvU+YBh5mZg836pPR41XoSX8+MMDMKsysI2GRY0ajPjOAMdHjrwMvebQiEmdcjeZ8LyLMyeaDGcDlUUXKUGCzu6+OOygz6103j2lmQwj/d7OeKKLXvA94y91/00y3nB+zdOKK45iZWZmZdY8edwbOBd5u1C3nf5PpxBXH36S73+Tufd29nJAnXnL3bzXqltHj1b6tP5gP3L3WzCYAzxMqZu539yVm9nOg2t1nEP4wfm9mNYSFwovzJK4qM7sIqI3iGpvtuADMbCqhqqPUzFYCNxMWtXD3u4BnCdUoNcA24Io8ievrwLVmVgtsBy7OweAN4UzsMuDNaD4Y4EfAESmxxXHM0okrjmN2GPCgmZUQBplH3f2ZuP8m04wrlr/JpmTzeOk2DCIiCVLo0zsiItIKSvoiIgmipC8ikiBK+iIiCaKkLyKSIEr6IiIJoqQvIpIg/x9Ag/jQQypYmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(unencode_history.history['val_loss'], \"r\")\n",
    "ax = plt.plot(encode_history.history['val_loss'], \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac9d9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.54927206805753\n",
      "22.58852330164964\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "print(np.sqrt(mse_encoded))\n",
    "print(np.sqrt(mse_unencoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89827ecf",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6a015",
   "metadata": {},
   "source": [
    "### Time Series Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a53764",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f116b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for spliting time series data alone\n",
    "\n",
    "def spliting_ts_train_test(train_ratio, \n",
    "                           val_ratio, \n",
    "                           test_ratio,\n",
    "                           data):\n",
    "    \n",
    "    train_size = int(train_ratio * len(data))\n",
    "    val_size = int(val_ratio * len(data))\n",
    "    test_size = len(data) - train_size - val_size\n",
    "    \n",
    "    ts_data = pd.DataFrame(data).reset_index()\n",
    "    ts_data = ts_data.rename(columns = {'day': 'ds', 'CORE_VL_NbEntry': 'y'})\n",
    "    \n",
    "    \n",
    "    ts_train = ts_data.iloc[:train_size]\n",
    "    ts_val = ts_data.iloc[train_size:train_size + val_size]\n",
    "    ts_test = ts_data.iloc[-test_size:]\n",
    "\n",
    "    return ts_train, ts_val, ts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a576109",
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding holiday indicator to prophet\n",
    "holiday_raw = pd.read_excel('/Users/yvonne_zhu/Desktop/MSDS Capstone/Capstone_EDAV/RData/US_holiday_labelled.xlsx')\n",
    "df_holiday = holiday_raw[['Date','Holiday']].rename(columns = {'Date': 'ds','Holiday': 'holiday'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b01fd789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet 1.1.2\n"
     ]
    }
   ],
   "source": [
    "# check prophet version\n",
    "import prophet\n",
    "# print version number\n",
    "print('Prophet %s' % prophet.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a1efe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:start chain 1\n",
      "INFO:cmdstanpy:finish chain 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<prophet.forecaster.Prophet at 0x7fd0e6722250>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prophet import Prophet \n",
    "prophet_model = Prophet(interval_width = 0.95, holidays=df_holiday, daily_seasonality=True)\n",
    "\n",
    "ts_train, ts_val, ts_test = spliting_ts_train_test(0.7, 0.2, 0.1, data)\n",
    "prophet_model.fit(ts_train[['ds','y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fada63ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.861650029452843"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output RMSE for model \n",
    "forecast_y = prophet_model.predict(ts_test[['ds']])\n",
    "mean_squared_error(ts_test['y'],forecast_y['yhat'],squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f9acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5a1628c",
   "metadata": {},
   "source": [
    "### LSTM model\n",
    "\n",
    "https://arxiv.org/pdf/2205.06673.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "005773aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /Users/yvonne_zhu/opt/anaconda3/lib/python3.8/site-packages (from keras_preprocessing) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/yvonne_zhu/opt/anaconda3/lib/python3.8/site-packages (from keras_preprocessing) (1.22.4)\n",
      "Installing collected packages: keras-preprocessing\n",
      "Successfully installed keras-preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# ! pip install keras_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae7cccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get ready\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report,roc_auc_score,confusion_matrix,accuracy_score,f1_score,roc_curve\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "\n",
    "import string\n",
    "\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee8ae4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vanilla LSTM\n",
    "\n",
    "# define model, training dataset [samples, timesteps, features]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(30, 137)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2165ab6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CORE_VL_NbEntry</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>Population</th>\n",
       "      <th>MHI</th>\n",
       "      <th>CORE_LB_State_AZ</th>\n",
       "      <th>CORE_LB_State_CA</th>\n",
       "      <th>CORE_LB_State_CO</th>\n",
       "      <th>CORE_LB_State_CT</th>\n",
       "      <th>CORE_LB_State_DC</th>\n",
       "      <th>...</th>\n",
       "      <th>weekday_Thursday</th>\n",
       "      <th>weekday_Tuesday</th>\n",
       "      <th>weekday_Wednesday</th>\n",
       "      <th>y</th>\n",
       "      <th>m</th>\n",
       "      <th>d</th>\n",
       "      <th>m_sin</th>\n",
       "      <th>m_cos</th>\n",
       "      <th>d_sin</th>\n",
       "      <th>d_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>40</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>65913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>71</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>22</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>52</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>25</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>84097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.97953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CORE_VL_NbEntry  year  month  Population    MHI  CORE_LB_State_AZ  \\\n",
       "day                                                                             \n",
       "2016-01-01               40  2016      1          62  65913                 1   \n",
       "2016-01-01               71  2016      1         253  84097                 0   \n",
       "2016-01-01               22  2016      1         253  84097                 0   \n",
       "2016-01-01               52  2016      1         253  84097                 0   \n",
       "2016-01-01               25  2016      1         253  84097                 0   \n",
       "\n",
       "            CORE_LB_State_CA  CORE_LB_State_CO  CORE_LB_State_CT  \\\n",
       "day                                                                \n",
       "2016-01-01                 0                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "2016-01-01                 1                 0                 0   \n",
       "\n",
       "            CORE_LB_State_DC  ...  weekday_Thursday  weekday_Tuesday  \\\n",
       "day                           ...                                      \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "2016-01-01                 0  ...                 0                0   \n",
       "\n",
       "            weekday_Wednesday     y  m  d  m_sin     m_cos     d_sin    d_cos  \n",
       "day                                                                            \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "2016-01-01                  0  2016  1  1    0.5  0.866025  0.201299  0.97953  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71f6b599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06700168],\n",
       "       [0.11892797],\n",
       "       [0.03685092],\n",
       "       ...,\n",
       "       [0.08877722],\n",
       "       [0.05527638],\n",
       "       [0.0519263 ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try on univariate data without transformation\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "lstm_data = min_max_scaler.fit_transform(data['CORE_VL_NbEntry'].values.reshape(-1,1))\n",
    "\n",
    "# note that lstm_data only contains y value but not multivariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d68584af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train, lstm_val, lstm_test = spliting_ts_train_test(0.7,0.2,0.1,lstm_data)\n",
    "\n",
    "# currently define look back date as 30 days\n",
    "\n",
    "def create_lookback(data, look_back=30):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(data) - look_back - 1):\n",
    "        a = data[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(data[i+look_back],0)\n",
    "    \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a777a242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.067002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.118928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.036851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.087102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.041876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         0\n",
       "0      0  0.067002\n",
       "1      1  0.118928\n",
       "2      2  0.036851\n",
       "3      3  0.087102\n",
       "4      4  0.041876"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c2e277ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'(slice(0, 30, None), 0)' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-e757ca1afb78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lookback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-99d1d54ed18f>\u001b[0m in \u001b[0;36mcreate_lookback\u001b[0;34m(data, look_back)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlook_back\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdataX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdataY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m             \u001b[0mcasted_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(0, 30, None), 0)' is an invalid key"
     ]
    }
   ],
   "source": [
    "x_train, y_train = create_lookback(lstm_train, look_back=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4528198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
